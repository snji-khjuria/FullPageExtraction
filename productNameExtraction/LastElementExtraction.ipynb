{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the corpus directory out of which we'll learn patterns: \n",
      "Corpus directory is featureSpecificationData/amazon/corpus\n",
      "Train Files are ['featureSpecificationData/amazon/corpus/page1', 'featureSpecificationData/amazon/corpus/page6', 'featureSpecificationData/amazon/corpus/page7', 'featureSpecificationData/amazon/corpus/page5', 'featureSpecificationData/amazon/corpus/page8', 'featureSpecificationData/amazon/corpus/page3']\n",
      "Test files are ['featureSpecificationData/amazon/corpus/page9', 'featureSpecificationData/amazon/corpus/page10', 'featureSpecificationData/amazon/corpus/page2', 'featureSpecificationData/amazon/corpus/page4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#corpus directory for flipkart\n",
    "# productNameData/flipkart/corpus\n",
    "print(\"Enter the corpus directory out of which we'll learn patterns: \")\n",
    "# corpusDirectory = raw_input()\n",
    "corpusDirectory = \"featureSpecificationData/amazon/corpus\"\n",
    "print(\"Corpus directory is \" + corpusDirectory)\n",
    "directories = getAllDirectoriesInLocation(corpusDirectory)\n",
    "(trainDirs, testDirs) = trainTestSplit(directories)\n",
    "print(\"Train Files are \" + str(trainDirs))\n",
    "print(\"Test files are \" + str(testDirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#do train/test split\n",
    "def trainTestSplit(files):\n",
    "    sixtyPercent = int(len(files)*(0.6))\n",
    "    if sixtyPercent<4:\n",
    "        print(\"We won't be able to report accuracy of patterns\")\n",
    "        return (files, [])\n",
    "    else:\n",
    "        return (files[:sixtyPercent], files[sixtyPercent:])\n",
    "\n",
    "def getAllDirectoriesInLocation(loc):\n",
    "    listOfSubDir = [loc+\"/\"+f for f in os.listdir(loc)]\n",
    "    return listOfSubDir\n",
    "\n",
    "\n",
    "def removeMultipleWhiteSpaces(s):\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "\n",
    "def getLastElementSeed(fileLocation):\n",
    "    with open(fileLocation) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    return removeMultipleWhiteSpaces(content[len(content)-1])\n",
    "\n",
    "\n",
    "def readAllSeedsAsSet(fileLocation):\n",
    "    with open(fileLocation) as f:\n",
    "        content = f.readlines()\n",
    "    content = [removeMultipleWhiteSpaces(x.strip()) for x in content]\n",
    "    return set(content)\n",
    "\n",
    "\n",
    "\n",
    "def getPageLocationAndSeed(d):\n",
    "    output = []\n",
    "    for pageDir in d:\n",
    "        pageLocation = pageDir + \"/page.html\"\n",
    "        seedLocation = pageDir + \"/seed\"\n",
    "        seed = getLastElementSeed(seedLocation)\n",
    "        output.append((pageLocation, seed))\n",
    "    return output\n",
    "\n",
    "\n",
    "#read the page from pageLocation\n",
    "def readPage(pageLocation):\n",
    "    htmlPageContent = \"\"\n",
    "    with open(pageLocation, 'r') as myfile:\n",
    "        htmlPageContent = myfile.read().strip()\n",
    "    return htmlPageContent\n",
    "\n",
    "def removeTerminatedAnd(s):\n",
    "    return re.sub(\"&amp;\", \"&\", s)\n",
    "\n",
    "\n",
    "\n",
    "#document processing logic at this stage is about removing multiple whitespaces into single one \n",
    "def preprocessDocument(document):\n",
    "    return removeTerminatedAnd(' '.join(document.split()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getLeftAndRightContexts(pageContent, seed):\n",
    "    b = getEachSeedLocationsInPage(pageContent, seed)\n",
    "    output = []\n",
    "    totalLength = len(pageContent)\n",
    "    leftContexts = []\n",
    "    rightContexts = []\n",
    "    for (s, e) in b:\n",
    "        leftContext = pageContent[max(0, s-100):s]\n",
    "        leftContext = leftContext[::-1]\n",
    "        rightContext = pageContent[e:min(totalLength, e+100)]\n",
    "        leftContexts.append(leftContext)\n",
    "        rightContexts.append(rightContext)\n",
    "    return (leftContexts, rightContexts)\n",
    "    \n",
    "    \n",
    "#find all start, end pairs of particular key\n",
    "def getAllStartEndPairs(document, key):\n",
    "    keyLength = len(key)\n",
    "    return [(i, i+keyLength) for i in range(len(document)) if document.startswith(key, i)]\n",
    " \n",
    "    \n",
    "#find seeds for one particular seed(at this stage call only getAllStartEndPairs())\n",
    "def getEachSeedLocationsInPage(htmlPageContent, seed):\n",
    "    return getAllStartEndPairs(htmlPageContent, seed)\n",
    "\n",
    "def printContextInformation(contexts, pageAndSeed, rev=False):\n",
    "    \n",
    "    for index in range(0, len(pageAndSeed)):\n",
    "        (pageLocation, seed) = pageAndSeed[index]\n",
    "        pageContent = preprocessDocument(readPage(pageLocation))\n",
    "        contextPerPage = contexts[index]\n",
    "        print(\"For seed \" + str(seed))\n",
    "        print(\"Context is following: \" + pageLocation + \" number \" + str(contextPerPage))\n",
    "        for item in contextPerPage:\n",
    "            if rev==True:\n",
    "                item = item[::-1]\n",
    "            print(item)\n",
    "            print(\"---------------------\")\n",
    "\n",
    "def getCommonPrefix(s1, s2):\n",
    "    l = [s1, s2]\n",
    "    return os.path.commonprefix(l)\n",
    "\n",
    "\n",
    "\n",
    "def insertCommonPrefix(results, commonPrefix):\n",
    "    if commonPrefix in results or len(commonPrefix)<=0:\n",
    "        return results\n",
    "    r = list(results)\n",
    "    for item in r:\n",
    "        if len(item)<len(commonPrefix) and commonPrefix.startswith(item):\n",
    "            results.remove(item)\n",
    "    r=list(results)\n",
    "    for item in r:\n",
    "        if len(item)>len(commonPrefix) and item.startswith(commonPrefix):\n",
    "            return results\n",
    "    results.add(commonPrefix)\n",
    "    return results\n",
    "\n",
    "def doPrefixIntersection(list1, list2):\n",
    "    results = set()\n",
    "    for item1 in list1:\n",
    "        commonPrefix = \"\"\n",
    "        for item2 in list2:\n",
    "            c = getCommonPrefix(item1, item2)\n",
    "            if len(c)>len(commonPrefix):\n",
    "                commonPrefix = c\n",
    "        results = insertCommonPrefix(results, commonPrefix)\n",
    "    return results\n",
    "\n",
    "\n",
    "def doIntersection(patterns):\n",
    "    if len(patterns)<=0:\n",
    "        return patterns\n",
    "    result = patterns[0]\n",
    "    for index in range(1, len(patterns)):\n",
    "        result = doPrefixIntersection(result, patterns[index])\n",
    "    return result\n",
    "\n",
    "\n",
    "def getLeftPatterns(leftContexts):\n",
    "    lp = doIntersection(leftContexts)\n",
    "    results = []\n",
    "    for item in lp:\n",
    "        results.append(item[::-1])\n",
    "    return results\n",
    "\n",
    "\n",
    "def getRightPatterns(rightContexts):\n",
    "    return list(doIntersection(rightContexts))\n",
    "\n",
    "\n",
    "def allPossiblePairs(leftPatterns, rightPatterns):\n",
    "    patterns = []\n",
    "    for lp in leftPatterns:\n",
    "        for rp in rightPatterns:\n",
    "            patterns.append((lp, rp))\n",
    "    return patterns\n",
    "\n",
    "#Pattern is (l, r) and match them to htmlPageContent\n",
    "def findEntitySetwrtPattern(htmlPageContent, (l, r)):\n",
    "    #for each start location of pattern find its end\n",
    "    #for each end page find the pattern right\n",
    "    #extract everything till that point\n",
    "    #after extraction move one point above that pattern string\n",
    "    results = []\n",
    "    for m in re.finditer(re.escape(l), htmlPageContent):\n",
    "        start = m.start()\n",
    "        end = m.end()\n",
    "        rightPage = htmlPageContent[end:]\n",
    "        rightLoc  = rightPage.find(r)\n",
    "        if rightLoc==-1:\n",
    "            break\n",
    "        element = rightPage[:rightLoc]\n",
    "        if len(element)>1 and len(element)<500:\n",
    "            results.append(element)\n",
    "    return set(results)\n",
    "\n",
    "\n",
    "def extractSet(patterns, htmlPageContent):\n",
    "    output = []\n",
    "    for pattern in patterns:\n",
    "        output.extend(findEntitySetwrtPattern(htmlPageContent, pattern))\n",
    "    return set(output)\n",
    "\n",
    "\n",
    "#remove elements which were actually tag\n",
    "def isTag(s):\n",
    "    if s.find(\"<\")!=-1 and s.find(\">\")!=-1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#preprocess results before writing it to file\n",
    "def preprocessResults(output):\n",
    "    result = []\n",
    "    for o in output:\n",
    "        if isTag(o):\n",
    "            continue\n",
    "        result.append(o)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filterPatterns(pageLocationAndSeeds, patterns, applyClass=False, regExpIn=\"\", regExpOut=\"\"):\n",
    "    seedsSet = set()\n",
    "    for (pageLoc, seed) in pageLocationAndSeeds:\n",
    "        for s in seed:\n",
    "            seedsSet.add(s)\n",
    "    seeds = list(seedsSet)\n",
    "    patternSuccess = []\n",
    "    for index in range(0, len(patterns)):\n",
    "        patternSuccess.append(0)\n",
    "    for index in range(0, len(patterns)):\n",
    "        for (pageLocation, seed) in pageLocationAndSeeds:\n",
    "            pageContent = preprocessDocument(readPage(pageLocation))\n",
    "            if applyClass == True:\n",
    "                pageContent = doProcessingWithoutClass(preprocessDocument(readPage(pageLocation)), regExpIn, regExpOut)\n",
    "            resultsFound = extractSet([patterns[index]], pageContent)\n",
    "            flag = True\n",
    "            for s in seed:\n",
    "                if not s in resultsFound:\n",
    "                    flag=False\n",
    "            if flag==True:\n",
    "                patternSuccess[index]+=1\n",
    "                \n",
    "    output = []\n",
    "    for index in range(0, len(patterns)):\n",
    "        if patternSuccess[index]>0:\n",
    "            output.append(patterns[index])\n",
    "    return output\n",
    "\n",
    "\n",
    "# def filterPatterns(pageLocationAndSeeds, patterns, applyClass=False):\n",
    "#     seedsSet = set()\n",
    "#     for (pageLoc, s) in pageLocationAndSeeds:\n",
    "#         seedsSet.add(s)\n",
    "#     seeds = list(seedsSet)\n",
    "#     patternSuccess = []\n",
    "#     for index in range(0, len(patterns)):\n",
    "#         patternSuccess.append(0)\n",
    "#     for index in range(0, len(patterns)):\n",
    "#         for (pageLocation, seed) in pageLocationAndSeeds:\n",
    "#             pageContent = preprocessDocument(readPage(pageLocation))\n",
    "#             if applyClass == True:\n",
    "#                 pageContent = doProcessingWithoutClass(pageContent)\n",
    "#             resultsFound = extractSet([patterns[index]], pageContent)\n",
    "#             if seed in resultsFound:\n",
    "#                 patternSuccess[index]+=1\n",
    "                \n",
    "#     output = []\n",
    "#     for index in range(0, len(patterns)):\n",
    "#         if patternSuccess[index]>0:\n",
    "#             output.append(patterns[index])\n",
    "#     return output\n",
    "\n",
    "\n",
    "# def filterPatterns(pageLocationAndSeeds, patterns, applyClass=False, regExpIn=\"\", regExpOut=\"\"):\n",
    "#     seedsSet = set()\n",
    "#     for (pageLoc, seed) in pageLocationAndSeeds:\n",
    "#         for s in seed:\n",
    "#             seedsSet.add(s)\n",
    "#     seeds = list(seedsSet)\n",
    "#     patternSuccess = []\n",
    "#     for index in range(0, len(patterns)):\n",
    "#         patternSuccess.append(0)\n",
    "#     for index in range(0, len(patterns)):\n",
    "#         for (pageLocation, seed) in pageLocationAndSeeds:\n",
    "#             pageContent = preprocessDocument(readPage(pageLocation))\n",
    "#             if applyClass == True:\n",
    "#                 pageContent = doProcessingWithoutClass(pageContent, regExpIn, regExpOut)\n",
    "#             resultsFound = extractSet([patterns[index]], pageContent)\n",
    "#             flag = True\n",
    "#             for s in seed:\n",
    "#                 if not s in resultsFound:\n",
    "#                     flag=False\n",
    "#             if flag==True:\n",
    "#                 patternSuccess[index]+=1\n",
    "                \n",
    "#     output = []\n",
    "#     for index in range(0, len(patterns)):\n",
    "#         if patternSuccess[index]>0:\n",
    "#             output.append(patterns[index])\n",
    "#     return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def doProcessingWithoutClass(s, regExpIn, regExpOut):\n",
    "    allManaged =  re.sub(regExpIn, regExpOut, s)\n",
    "    removeQuoteIn = r'\"[^\"]*\"'\n",
    "    removeQuoteOut = r'\"\"'\n",
    "    return re.sub(removeQuoteIn, removeQuoteOut, allManaged)\n",
    "#     i = \"class[\\s]*=[\\s]*\"\n",
    "#     o       = \"class=\"\n",
    "#     s = re.sub(i, o, s)\n",
    "#     i = \"([^class])=\\\"[^\\\"]*\\\"\"\n",
    "#     o = \"\\\\1=\\\"\\\"\"\n",
    "#     return re.sub(i, o, s)\n",
    "\n",
    "\n",
    "\n",
    "def getLastWordWithoutEqualSign(leftContext):\n",
    "    words = leftContext.strip().split(\" \")\n",
    "    totalWords = len(words)\n",
    "    if len(words)<2:\n",
    "        return \"\"\n",
    "    else:\n",
    "        if words[totalWords-1]==\"=\":\n",
    "            return words[totalWords-2]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "def getSpecialWords(document, seedsSet):\n",
    "    totalLength = len(document)\n",
    "    specialWords = []\n",
    "    for s in seedsSet:\n",
    "        allSeedPositions = getAllStartEndPairs(document, s)\n",
    "        for (start, end) in allSeedPositions:\n",
    "            prevLoc = start-1\n",
    "            nextLoc = end\n",
    "            if prevLoc>=0 and nextLoc<totalLength:\n",
    "                prevQuot = document[prevLoc]\n",
    "                nextQuot  = document[nextLoc]\n",
    "#                 print(prevQuot + \" \" + nextQuot)\n",
    "                if prevQuot==\"\\\"\" and nextQuot==\"\\\"\":\n",
    "                    leftContext = document[max(0, prevLoc-100):prevLoc]\n",
    "                    lastWord = getLastWordWithoutEqualSign(leftContext)\n",
    "                    if len(lastWord)>0:\n",
    "                        specialWords.append(lastWord)\n",
    "    return specialWords\n",
    "\n",
    "\n",
    "\n",
    "def createRegExpFromWordStr(wordsStr):\n",
    "#     wordsStr = getWordsStr(words)\n",
    "#     print(\"wordsstr is \" + wordsStr)\n",
    "    return (\"(\" + wordsStr +\")[\\s]*=[\\s]*\\\"([^\\\"]*)\\\"\", \"\\\\1=\\\\2\")\n",
    "\n",
    "\n",
    "\n",
    "def getWordsStr(words):\n",
    "    out = words[0]\n",
    "    if len(words)==1:\n",
    "        return out\n",
    "    else:\n",
    "        for index in range(1, len(words)):\n",
    "            out+=\"|\"+words[index]\n",
    "    print(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def getAllSpecialWordsAsString(pageLocationAndSeed):\n",
    "    specialWords = []\n",
    "    specialWords.append(\"class\")\n",
    "    for (pageLocation, seeds) in pageLocationAndSeed:\n",
    "        pageContent = preprocessDocument(readPage(pageLocation))\n",
    "        specialWords.extend(getSpecialWords(pageContent, seeds))\n",
    "    specialWords = list(set(specialWords))\n",
    "    return getWordsStr(specialWords)\n",
    "\n",
    "\n",
    "\n",
    "def getAllPatternsByPlainStringMatch(corpusLocation):\n",
    "    directories             = getAllDirectoriesInLocation(corpusLocation)\n",
    "    (trainDirs, testDirs)   = trainTestSplit(directories)\n",
    "    pageLocationsAndSeed    = getPageLocationAndSeed(trainDirs)\n",
    "    testPageLocationAndSeed = getPageLocationAndSeed(testDirs)\n",
    "    leftContexts  = []\n",
    "    rightContexts = []\n",
    "    for (pageLocation, seed) in pageLocationsAndSeed:\n",
    "        pageContent = preprocessDocument(readPage(pageLocation))\n",
    "        (leftContextsPerPage, rightContextsPerPage) = getLeftAndRightContexts(pageContent, seed)\n",
    "        leftContexts.append(leftContextsPerPage)\n",
    "        rightContexts.append(rightContextsPerPage)\n",
    "#     printContextInformation(leftContexts, pageLocationsAndSeed, True)\n",
    "#     printContextInformation(rightContexts, pageLocationsAndSeed)\n",
    "    leftPatterns = getLeftPatterns(leftContexts)\n",
    "    rightPatterns = getRightPatterns(rightContexts)\n",
    "    \n",
    "#     print(\"Left patterns are \" + str(leftPatterns))\n",
    "#     print(\"Right patterns are \" + str(rightPatterns))\n",
    "    patterns = allPossiblePairs(leftPatterns, rightPatterns)\n",
    "#     print(\"All possible patterns: \")\n",
    "#     print(patterns)\n",
    "#     patterns = filterPatterns(pageLocationsAndSeed, patterns)\n",
    "    #TODO if number of patterns are empty go for empty class thing\n",
    "#     print(\"Final Patterns \")\n",
    "#     print(patterns)\n",
    "    recall = 0\n",
    "    extraResults = 0\n",
    "    for (pageLocation, seed) in testPageLocationAndSeed:\n",
    "        pageContent = preprocessDocument(readPage(pageLocation))\n",
    "        resultsPerPage = preprocessResults(list(extractSet(patterns, pageContent)))\n",
    "#         resultsPerPage = list(set(resultsPerPage))\n",
    "        if seed in resultsPerPage:\n",
    "            recall += 1\n",
    "        if len(resultsPerPage)>1:\n",
    "            seedsLocation = os.path.dirname(pageLocation) + \"/seed\"\n",
    "#             print(\"Seeds location is \" + str(seedsLocation))\n",
    "            allSeeds = readAllSeedsAsSet(seedsLocation)\n",
    "            totalCount = len(resultsPerPage)\n",
    "            allSeedsCount = len(list(allSeeds.intersection(set(resultsPerPage))))\n",
    "            extracount = totalCount - allSeedsCount\n",
    "            extraResults += extracount\n",
    "#         print(\"Results per page \" + str(resultsPerPage))\n",
    "    seedsMissed = len(testPageLocationAndSeed)-recall\n",
    "#     print(\"Seeds missed: \" + str(seedsMissed))\n",
    "#     print(\"Extra junk: \"+ str(extraResults))\n",
    "    return (patterns, seedsMissed, extraResults, len(testDirs))\n",
    "    \n",
    "# getAllPatternsByPlainStringMatch(corpusDirectory)\n",
    "def getAllPatternsByRetainingOnlyClassValue(corpusLocation):\n",
    "    directories             = getAllDirectoriesInLocation(corpusLocation)\n",
    "    (trainDirs, testDirs)   = trainTestSplit(directories)\n",
    "    pageLocationsAndSeed    = getPageLocationAndSeed(trainDirs)\n",
    "    testPageLocationAndSeed = getPageLocationAndSeed(testDirs)\n",
    "    specialWordsString      = getAllSpecialWordsAsString(pageLocationsAndSeed)\n",
    "    (regExpIn, regExpOut)   = createRegExpFromWordStr(specialWordsString)\n",
    "    leftContexts  = []\n",
    "    rightContexts = []\n",
    "    for (pageLocation, seed) in pageLocationsAndSeed:\n",
    "        pageContent = doProcessingWithoutClass(preprocessDocument(readPage(pageLocation)), regExpIn, regExpOut)\n",
    "        (leftContextsPerPage, rightContextsPerPage) = getLeftAndRightContexts(pageContent, seed)\n",
    "        leftContexts.append(leftContextsPerPage)\n",
    "        rightContexts.append(rightContextsPerPage)\n",
    "#     printContextInformation(leftContexts, pageLocationsAndSeed, True)\n",
    "#     printContextInformation(rightContexts, pageLocationsAndSeed)\n",
    "    leftPatterns = getLeftPatterns(leftContexts)\n",
    "    rightPatterns = getRightPatterns(rightContexts)\n",
    "    \n",
    "#     print(\"Left patterns are \" + str(leftPatterns))\n",
    "#     print(\"Right patterns are \" + str(rightPatterns))\n",
    "    patterns = allPossiblePairs(leftPatterns, rightPatterns)\n",
    "#     print(\"All possible patterns: \")\n",
    "#     print(patterns)\n",
    "#     patterns = filterPatterns(pageLocationsAndSeed, patterns, True, regExpIn, regExpOut)\n",
    "    #TODO if number of patterns are empty go for empty class thing\n",
    "#     print(\"Final Patterns \")\n",
    "#     print(patterns)\n",
    "    recall = 0\n",
    "    extraResults = 0\n",
    "    for (pageLocation, seed) in testPageLocationAndSeed:\n",
    "        pageContent = doProcessingWithoutClass(preprocessDocument(readPage(pageLocation)), regExpIn, regExpOut)\n",
    "        resultsPerPage = preprocessResults(list(extractSet(patterns, pageContent)))\n",
    "        if seed in resultsPerPage:\n",
    "            recall += 1\n",
    "#         else:\n",
    "#             print(\"Page location: \" + pageLocation)\n",
    "#             print(\"Seed is \" + seed)\n",
    "#             print(resultsPerPage)\n",
    "        if len(resultsPerPage)>1:\n",
    "            seedsLocation = os.path.dirname(pageLocation) + \"/seed\"\n",
    "            allSeeds = readAllSeedsAsSet(seedsLocation)\n",
    "            totalCount = len(resultsPerPage)\n",
    "            allSeedsCount = len(list(allSeeds.intersection(set(resultsPerPage))))\n",
    "            extracount = totalCount - allSeedsCount\n",
    "            extraResults += extracount\n",
    "#     print(\"number of pages\" + str(len(testPageLocationAndSeed)))\n",
    "    seedsMissed = len(testPageLocationAndSeed)-recall\n",
    "#     print(\"Seeds missed: \" + str(seedsMissed))\n",
    "#     print(\"Extra junk: \"+ str(extraResults))\n",
    "    return (patterns, seedsMissed, extraResults, len(testDirs), specialWordsString)\n",
    "    \n",
    "# getAllPatternsByRetainingOnlyClassValue(corpusDirectory)\n",
    "def writeListToFile(loc, l):\n",
    "    with open(loc, 'w') as f:\n",
    "        for item in l:\n",
    "            f.write(item+\"\\n\")\n",
    "\n",
    "            \n",
    "def escapeAllDoubleQuote(s):\n",
    "    return re.sub(\"\\\"\", \"\\\\\\\"\", s)\n",
    "            \n",
    "def writeProductPatternsToFile(fileLocation, productPatterns):\n",
    "    header = \"GroupId\\tWebsiteName\\tPatternType\\tMissed\\tJunk\\tOutof\\tLeftPattern\\tRightPattern\"\n",
    "    groupId = 1\n",
    "    output = []\n",
    "    output.append(header)\n",
    "    for (website, pType, patterns, missed, junk, total) in productPatterns:\n",
    "        rowPrefix = str(groupId) + \"\\t\" + website + \"\\t\" + pType\n",
    "        rowPrefix+=\"\\t\" + str(missed) + \"\\t\" + str(junk) + \"\\t\" + str(total)\n",
    "        for (l, r) in patterns:\n",
    "            row = rowPrefix + \"\\t\" + l + \"\\t\" + r\n",
    "            output.append(row)\n",
    "        groupId+=1\n",
    "#     print(\"output is \")\n",
    "#     for item in output:\n",
    "#         print(item)\n",
    "    writeListToFile(fileLocation, output)\n",
    "    print(\"Output written at location: \" + fileLocation)\n",
    "\n",
    "def getPatternStore(eCommerceDataSetLocation):\n",
    "    allWebsites = getAllDirectoriesInLocation(eCommerceDataSetLocation)\n",
    "#     print(\"All websites are \" + str(allWebsites))\n",
    "    patternsStore = []\n",
    "    for website in allWebsites:\n",
    "        corpusLocation = website + \"/corpus\"\n",
    "        websiteName = os.path.basename(website)\n",
    "        (plainPatterns, plainMissed, plainJunkCount, plainTotal) = getAllPatternsByPlainStringMatch(corpusLocation)\n",
    "        (noValPatterns, noValMissed, noValJunkCount, noValTotal, specialWords) = getAllPatternsByRetainingOnlyClassValue(corpusLocation)\n",
    "        if plainMissed <= noValMissed:\n",
    "            patterns = (websiteName, \"plainPattern\", plainPatterns, plainMissed, plainJunkCount, plainTotal)\n",
    "            print(plainPatterns)\n",
    "        else:\n",
    "            patterns = (websiteName, \"noValuePattern_\"+specialWords, noValPatterns, noValMissed, noValJunkCount, noValTotal)\n",
    "        patternsStore.append(patterns)\n",
    "    for p in patternsStore:\n",
    "        print(p)\n",
    "    return patternsStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter e-commerce data location: \n",
      "categoryData\n",
      "Enter outputLocation\n",
      "newPatternsLearnt\n",
      "Patterns will be found at newPatternsLearnt\n",
      "[('1z\" fill=\"#fff\" class=\"_24NaUy\"></path></svg></div><div class=\"_1HEvv0\"><div class=\"_1KHd47 Bomkwu\">', '</div><svg width=\"16\" height=\"27\" viewBox=\"0 0 16 27\" xmlns=\"http://www.w3.org/2000/svg\" class=\"_2XP')]\n",
      "[('\"', ' </span></li> </ul> </div> </div> <'), ('ss=\"a-list-item a-color-tertiary\"> \\xe2\\x80\\xba </span></li> <li><span class=\"a-list-item a-color-tertiary\"> ', ' </span></li> </ul> </div> </div> <')]\n",
      "('flipkart', 'plainPattern', [('1z\" fill=\"#fff\" class=\"_24NaUy\"></path></svg></div><div class=\"_1HEvv0\"><div class=\"_1KHd47 Bomkwu\">', '</div><svg width=\"16\" height=\"27\" viewBox=\"0 0 16 27\" xmlns=\"http://www.w3.org/2000/svg\" class=\"_2XP')], 1, 0, 4)\n",
      "('amazon', 'plainPattern', [('\"', ' </span></li> </ul> </div> </div> <'), ('ss=\"a-list-item a-color-tertiary\"> \\xe2\\x80\\xba </span></li> <li><span class=\"a-list-item a-color-tertiary\"> ', ' </span></li> </ul> </div> </div> <')], 0, 8, 4)\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 21] Is a directory: 'newPatternsLearnt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-30ff8303e001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Patterns will be found at \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputLocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpatternsStore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPatternStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meCommerceDataLocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwriteProductPatternsToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputLocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatternsStore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#featureSpecificationData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#categoryData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-10d5c3c54a20>\u001b[0m in \u001b[0;36mwriteProductPatternsToFile\u001b[0;34m(fileLocation, productPatterns)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;31m#     for item in output:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;31m#         print(item)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0mwriteListToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileLocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output written at location: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileLocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-10d5c3c54a20>\u001b[0m in \u001b[0;36mwriteListToFile\u001b[0;34m(loc, l)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;31m# getAllPatternsByRetainingOnlyClassValue(corpusDirectory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwriteListToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 21] Is a directory: 'newPatternsLearnt'"
     ]
    }
   ],
   "source": [
    "print(\"Enter e-commerce data location: \")\n",
    "eCommerceDataLocation = raw_input()\n",
    "print(\"Enter outputLocation\")\n",
    "outputLocation = raw_input()\n",
    "print(\"Patterns will be found at \" + str(outputLocation))\n",
    "patternsStore = getPatternStore(eCommerceDataLocation)\n",
    "writeProductPatternsToFile(outputLocation, patternsStore)\n",
    "#featureSpecificationData\n",
    "#categoryData\n",
    "#newPatternsLearnt/lastCategories.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
